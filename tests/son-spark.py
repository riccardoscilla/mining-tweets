from pyspark import SparkContext, SparkConf
from collections import defaultdict


def findFrequentItemsets(input, numPartitions, s, sc):
    data = sc.textFile(input, numPartitions)

    numPartitions = data.getNumPartitions()

    count = data.count()

    threshold = s*count

    #split string baskets into lists of items
    baskets = data.map(lambda line: line.split(',')) \
                  .map(lambda line: [  int(line[0]), [y for y in line[1].split(' ')]  ])

    #treat a basket as a set for fast check if candidate belongs
    basketSets = baskets.map(lambda line: [line[0], set(line[1])]  ).persist()

    print(basketSets.collect())

    time = 2
    # each worker calculates the itemsets of his partition
    localItemSets = baskets.mapPartitions(lambda data: [x for y in get_frequent_items_sets(data, threshold/numPartitions, time/numPartitions).values() for x in y], True)

    # #for reducing by key later
    # allItemSets = localItemSets.map(lambda n_itemset: (n_itemset,1))

    # #merge candidates that are equal, but generated by different workers
    # mergedCandidates = allItemSets.reduceByKey(lambda x,y: x).map(lambda x: x)

    # #distribute global candidates to all workers
    # mergedCandidates = mergedCandidates.collect()
    # candidates = sc.broadcast(mergedCandidates)

    # # print(candidates.value)

    # #count actual occurrence of candidates in document
    # counts = basketSets.flatMap(lambda line: [candidate for candidate in candidates.value if line.issuperset(candidate[0])])

    # # print(counts.collect())

    # #filter finalists
    # finalItemSets = counts.reduceByKey(lambda v1, v2: v1+v2).filter(lambda v: v[1]>=threshold)

    # # print(finalItemSets.collect())

    # #put into nice format
    # finalItemSets = finalItemSets.map(lambda row: ", ".join([str(x) for x in row[0]])+"\t("+str(row[1])+")")

    # # print(finalItemSets.collect())
    # log_results(finalItemSets.collect())


def log_results(itemsets):
    for i in itemsets:
        print(i)

def get_frequent_items_sets(data,min_sup,min_t,steps=0):
    # we transform the dataset in a list of sets
    transactions = list()

	# Temporary dictionary to count occurrences
    items = defaultdict(lambda: 0)

    # Temporary dictionary to store time
    time = defaultdict(lambda: [])

	# Returned dictionary
    solution = dict()
    L_set = set()

    # Fills transactions and counts "singletons"
    for line in data:
        transaction = set(line)
        transactions.append(transaction)
        for element in transaction:
            items[element]+=1

    # Add to the solution all frequent items
    for item, count in items.items():
        if count >= min_sup:
            L_set.add(frozenset([item]))

    # Generalize the steps it ends when there are no candidates
    # or if the user provided as input the number of parameters
    k = 2
    solution[k-1] = L_set
    while L_set != set([]) and k != steps+1:
        L_set = create_candidates(L_set,k)
        C_set = frequent_items(L_set,transactions,min_sup)
        if C_set != set([]): solution[k]=C_set
        L_set = C_set
        k = k + 1
    return solution

def create_candidates(itemSet, length):
        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])

def frequent_items(items, transactions, min_sup):
	_itemSet = set()
	counter = defaultdict(int)
	localDict = defaultdict(int)
	for item in items:
		for transaction in transactions:
			if item.issubset(transaction):localDict[item] += 1

	for item, count in localDict.items():
		if count >= min_sup:
			_itemSet.add(item)
	return _itemSet

if __name__ == "__main__":

    APP_NAME = "SON-apriori"

    conf = SparkConf().setAppName(APP_NAME)
    conf = conf.setMaster("local[*]")

    sc  = SparkContext(conf=conf)

    f_input = "prova.csv"
    threshold = float(0.75)

    numPartitions = 5

    findFrequentItemsets(f_input, numPartitions, threshold, sc)